{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550ba6d1-6d8d-4b92-9c75-d7ac7f3cb57c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249fc139-3391-4e09-a255-9f4a7ebc906d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Physics-informed learning of governing equations from scarce data\n",
    "# Zhao Chen, Yang Liu, and Hao Sun\n",
    "# 2021. Northeastern University\n",
    "\n",
    "# Please run RD_Pretrain_ADO and RDEq_ID in order.\n",
    "# =============================================================================\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.spatial import distance\n",
    "import time\n",
    "#import sobol_seq\n",
    "from pyDOE import lhs\n",
    "import pandas as pd\n",
    "import process_data\n",
    "import importlib\n",
    "importlib.reload(process_data)\n",
    "from process_data import *\n",
    "import os\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "mpl.rcParams['figure.figsize']=(12,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aedb95-a4f6-41c0-8829-74278584794c",
   "metadata": {},
   "source": [
    "# Variables definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc0bfd-a7cf-4dca-819d-859863445e91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# global N_FRAMES\n",
    "\n",
    "\n",
    "# global data_out_folder\n",
    "# global images_out_folder\n",
    "# global data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d79f4b-f158-4a8c-8f96-e6ccaf25192a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_FRAMES = 30\n",
    "ADO_CYCLES = 1 # 6\n",
    "Pre_Adam_iter = 100 #10_000\n",
    "Adam_iter = 1_00\n",
    "Pretrain_lbfgs_max_iter = 1_00 # 40_000\n",
    "lbfgs_max_iter = 1_00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67934f-4a89-4e27-8693-c5d9d47d4a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_out_folder = \"output_test/output_data\"\n",
    "images_out_folder = \"output_test/output_images\"\n",
    "data_file = \"data/vd_truth_train_200.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eecfec-a5a3-46bd-8668-791f932d4ab2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf54778-d4bd-4a23-a8ca-74b40d5be3f1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    \n",
    "    # Adam loss history(Pretraining)\n",
    "    loss_history_Adam_Pretrain = np.empty([0])\n",
    "    loss_val_history_Adam_Pretrain = np.empty([0])\n",
    "    loss_u_history_Adam_Pretrain = np.empty([0])\n",
    "    loss_v_history_Adam_Pretrain = np.empty([0])\n",
    "    loss_f_u_history_Adam_Pretrain = np.empty([0])\n",
    "    loss_f_v_history_Adam_Pretrain = np.empty([0])\n",
    "    loss_lambda_u_history_Adam_Pretrain = np.empty([0])\n",
    "    loss_lambda_v_history_Adam_Pretrain = np.empty([0])\n",
    "    lambda_u_history_Adam_Pretrain = np.zeros((110,1))\n",
    "    lambda_v_history_Adam_Pretrain = np.zeros((110,1))\n",
    "    \n",
    "    # L-BFGS-B loss history(Pretraining)\n",
    "    loss_history_Pretrain = np.empty([0])\n",
    "    loss_val_history_Pretrain = np.empty([0])\n",
    "    loss_u_history_Pretrain = np.empty([0])\n",
    "    loss_v_history_Pretrain = np.empty([0])\n",
    "    loss_f_u_history_Pretrain = np.empty([0])\n",
    "    loss_f_v_history_Pretrain = np.empty([0])\n",
    "    loss_lambda_u_history_Pretrain = np.empty([0])\n",
    "    loss_lambda_v_history_Pretrain = np.empty([0])\n",
    "    lambda_u_history_Pretrain = np.zeros((110,1))\n",
    "    lambda_v_history_Pretrain = np.zeros((110,1))    \n",
    "    step_Pretrain = 0\n",
    "    \n",
    "    # L-BFGS-S loss history\n",
    "    loss_history = np.empty([0])\n",
    "    loss_val_history = np.empty([0])\n",
    "    loss_u_history = np.empty([0])\n",
    "    loss_v_history = np.empty([0])\n",
    "    loss_f_u_history = np.empty([0])\n",
    "    loss_f_v_history = np.empty([0])\n",
    "    loss_lambda_u_history = np.empty([0])\n",
    "    loss_lambda_v_history = np.empty([0])\n",
    "    lambda_u_history = np.zeros((110,1))\n",
    "    lambda_v_history = np.zeros((110,1))    \n",
    "    step = 0\n",
    "    \n",
    "    # Adam loss history\n",
    "    loss_history_Adam = np.empty([0])\n",
    "    loss_val_history_Adam = np.empty([0])\n",
    "    loss_u_history_Adam = np.empty([0])\n",
    "    loss_v_history_Adam = np.empty([0])\n",
    "    loss_f_u_history_Adam = np.empty([0])\n",
    "    loss_f_v_history_Adam = np.empty([0])\n",
    "    loss_lambda_u_history_Adam = np.empty([0])\n",
    "    loss_lambda_v_history_Adam = np.empty([0])\n",
    "    lambda_u_history_Adam = np.zeros((110,1))\n",
    "    lambda_v_history_Adam = np.zeros((110,1))\n",
    "    \n",
    "    # Alter loss history\n",
    "    loss_history_Alter = np.empty([0])\n",
    "    loss_val_history_Alter = np.empty([0])\n",
    "    loss_u_history_Alter = np.empty([0])\n",
    "    loss_v_history_Alter = np.empty([0])\n",
    "    loss_f_u_history_Alter = np.empty([0])\n",
    "    loss_f_v_history_Alter = np.empty([0])\n",
    "    loss_lambda_u_history_Alter = np.empty([0])\n",
    "    loss_lambda_v_history_Alter = np.empty([0])\n",
    "    lambda_u_history_Alter = np.zeros((110,1))\n",
    "    lambda_v_history_Alter = np.zeros((110,1))\n",
    "    \n",
    "    # STRidge loss histroy\n",
    "    loss_u_history_STRidge = np.empty([0])\n",
    "    loss_f_u_history_STRidge = np.empty([0])\n",
    "    loss_lambda_u_history_STRidge = np.empty([0])\n",
    "    tol_u_history_STRidge = np.empty([0])\n",
    "    lambda_u_history_STRidge = np.zeros((110, 1))\n",
    "    ridge_u_append_counter_STRidge = np.array([0])\n",
    "\n",
    "    loss_v_history_STRidge = np.empty([0])\n",
    "    loss_f_v_history_STRidge = np.empty([0])\n",
    "    loss_lambda_v_history_STRidge = np.empty([0])\n",
    "    tol_v_history_STRidge = np.empty([0])\n",
    "    lambda_v_history_STRidge = np.zeros((110, 1))\n",
    "    ridge_v_append_counter_STRidge = np.array([0])\n",
    "        \n",
    "    lib_fun = []\n",
    "    lib_descr = []\n",
    "    \n",
    "    \n",
    "    np.random.seed(1234)\n",
    "    tf.set_random_seed(1234)\n",
    "    \n",
    "    class PhysicsInformedNN:\n",
    "# =============================================================================\n",
    "#     Inspired by Raissi, Maziar, Paris Perdikaris, and George E. Karniadakis.\n",
    "#     \"Physics-informed neural networks: A deep learning framework for solving forward and inverse problems\n",
    "#     involving nonlinear partial differential equations.\" Journal of Computational Physics 378 (2019): 686-707.\n",
    "# =============================================================================\n",
    "        # Initialize the class\n",
    "        def __init__(self, X, u, v, X_f, X_val, u_val, v_val, layers, lb, ub, BatchNo):\n",
    "            \n",
    "            self.lb = lb.astype(np.float32)\n",
    "            self.ub = ub.astype(np.float32)\n",
    "            self.layers = layers\n",
    "            \n",
    "            # Initialize NNs\n",
    "            self.weights, self.biases = self.initialize_NN(layers)\n",
    "            \n",
    "            config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "            # config.gpu_options.allow_growth = True\n",
    "            self.sess = tf.Session(config = config)\n",
    "            \n",
    "            # Initialize parameters\n",
    "            self.lambda_u = tf.Variable(tf.zeros([110, 1], dtype=tf.float32), dtype=tf.float32, name = 'lambda_u')\n",
    "            self.lambda_v = tf.Variable(tf.zeros([110, 1], dtype=tf.float32), dtype=tf.float32, name = 'lambda_v')\n",
    "            \n",
    "            # Specify the list of trainable variables \n",
    "            var_list_1 = self.biases + self.weights\n",
    "            \n",
    "            var_list_Pretrain = self.biases + self.weights\n",
    "            var_list_Pretrain.append(self.lambda_u)\n",
    "            var_list_Pretrain.append(self.lambda_v)\n",
    "            \n",
    "            ######### Training data ################\n",
    "            self.x = X[:,0:1]\n",
    "            self.y = X[:,1:2]\n",
    "            self.t = X[:,2:3]\n",
    "            self.u = u\n",
    "            self.v = v\n",
    "            # Collocation points\n",
    "            self.x_f = X_f[:,0:1]\n",
    "            self.y_f = X_f[:,1:2]\n",
    "            self.t_f = X_f[:,2:3]\n",
    "            \n",
    "            self.BatchNo = BatchNo\n",
    "            self.batchsize_train = np.floor(self.x.shape[0]/self.BatchNo)\n",
    "            self.batchsize_f = np.floor(self.x_f.shape[0]/self.BatchNo)            \n",
    "            \n",
    "            self.x_tf = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n",
    "            self.y_tf = tf.placeholder(tf.float32, shape=[None, self.y.shape[1]])\n",
    "            self.t_tf = tf.placeholder(tf.float32, shape=[None, self.t.shape[1]])\n",
    "            self.u_tf = tf.placeholder(tf.float32, shape=[None, self.u.shape[1]])\n",
    "            self.v_tf = tf.placeholder(tf.float32, shape=[None, self.v.shape[1]])\n",
    "            self.x_f_tf = tf.placeholder(tf.float32, shape=[None, self.x_f.shape[1]], name = 'x_f_tf')\n",
    "            self.y_f_tf = tf.placeholder(tf.float32, shape=[None, self.y_f.shape[1]])\n",
    "            self.t_f_tf = tf.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])\n",
    "            \n",
    "            self.u_pred, self.v_pred = self.net_uv(self.x_tf, self.y_tf, self.t_tf)\n",
    "            self.f_u_pred, self.f_v_pred, self.Phi, self.u_t_pred, self.v_t_pred = self.net_f(self.x_f_tf, self.y_f_tf, self.t_f_tf,\n",
    "                                                                                              self.batchsize_f)\n",
    "            \n",
    "            self.loss_u = tf.reduce_mean(tf.abs(self.u_tf - self.u_pred))   # modified tf.sqare to tf.abs\n",
    "            self.loss_v = tf.reduce_mean(tf.abs(self.v_tf - self.v_pred))   # modified tf.sqare to tf.abs\n",
    "            \n",
    "            self.loss_f_coeff_tf = tf.placeholder(tf.float32)\n",
    "            \n",
    "            self.loss_f_u = self.loss_f_coeff_tf*tf.reduce_mean(tf.square(self.f_u_pred))\n",
    "            self.loss_f_v = self.loss_f_coeff_tf*tf.reduce_mean(tf.square(self.f_v_pred))\n",
    "            \n",
    "            self.loss_lambda_u = 1e-7*tf.norm(self.lambda_u, ord=1)    \n",
    "            self.loss_lambda_v = 1e-7*tf.norm(self.lambda_v, ord=1)    \n",
    "                        \n",
    "            self.loss = tf.log(self.loss_u  + self.loss_v + self.loss_f_u + self.loss_f_v + self.loss_lambda_u + self.loss_lambda_v) \n",
    "                        \n",
    "            ######### Validation data ################\n",
    "            self.x_val = X_val[:,0:1]\n",
    "            self.y_val = X_val[:,1:2]\n",
    "            self.t_val = X_val[:,2:3]\n",
    "            self.u_val = u_val\n",
    "            self.v_val = v_val\n",
    "            \n",
    "            self.batchsize_val = np.floor(self.x_val.shape[0]/self.BatchNo)\n",
    "            \n",
    "            self.x_val_tf = tf.placeholder(tf.float32, shape=[None, self.x_val.shape[1]])\n",
    "            self.y_val_tf = tf.placeholder(tf.float32, shape=[None, self.y_val.shape[1]])\n",
    "            self.t_val_tf = tf.placeholder(tf.float32, shape=[None, self.t_val.shape[1]])\n",
    "            self.u_val_tf = tf.placeholder(tf.float32, shape=[None, self.u_val.shape[1]])\n",
    "            self.v_val_tf = tf.placeholder(tf.float32, shape=[None, self.v_val.shape[1]])\n",
    "            \n",
    "            self.u_val_pred, self.v_val_pred = self.net_uv(self.x_val_tf, self.y_val_tf, self.t_val_tf)\n",
    "    \n",
    "            self.loss_u_val = tf.reduce_mean(tf.abs(self.u_val_tf - self.u_val_pred))   # modified tf.sqare to tf.abs\n",
    "            self.loss_v_val = tf.reduce_mean(tf.abs(self.v_val_tf - self.v_val_pred))   # modified tf.sqare to tf.abs\n",
    "            self.loss_val = tf.log(self.loss_u_val  + self.loss_v_val)     \n",
    "            \n",
    "            ######### Optimizor #########################\n",
    "            self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
    "                                                                    var_list = var_list_1,\n",
    "                                                                    method = 'L-BFGS-B', \n",
    "                                                                    options = {'maxiter': lbfgs_max_iter, #1_000\n",
    "                                                                               'maxfun': 1_000, \n",
    "                                                                               'maxcor': 50,\n",
    "                                                                               'maxls': 50,\n",
    "                                                                               'ftol' : 0.1 * np.finfo(float).eps})\n",
    "    \n",
    "            self.optimizer_Pretrain = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
    "                                                                            var_list = var_list_Pretrain,\n",
    "                                                                            method = 'L-BFGS-B', \n",
    "                                                                            options = {'maxiter': Pretrain_lbfgs_max_iter, # 40_000\n",
    "                                                                                       'maxfun': 1000, \n",
    "                                                                                       'maxcor': 50,\n",
    "                                                                                       'maxls': 50,\n",
    "                                                                                       'ftol' : 0.1 * np.finfo(float).eps})\n",
    "            \n",
    "            self.optimizer_Adam = tf.contrib.opt.NadamOptimizer(learning_rate = 1e-3) \n",
    "            self.train_op_Adam = self.optimizer_Adam.minimize(self.loss, var_list = var_list_1)\n",
    "                        \n",
    "            self.optimizer_Adam_Pretrain = tf.contrib.opt.NadamOptimizer(learning_rate = 3e-3) #5e-3\n",
    "            self.train_op_Adam_Pretrain = self.optimizer_Adam_Pretrain.minimize(self.loss, var_list = var_list_Pretrain)\n",
    "                        \n",
    "            # Save the model after pretraining\n",
    "            self.saver = tf.train.Saver(var_list_Pretrain)\n",
    "            \n",
    "            init = tf.global_variables_initializer()\n",
    "            self.sess.run(init)\n",
    "    \n",
    "        def initialize_NN(self, layers):        \n",
    "            weights = []\n",
    "            biases = []\n",
    "            num_layers = len(layers) \n",
    "            for l in range(0,num_layers-1):\n",
    "                W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "                b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32, name = 'b')\n",
    "                weights.append(W)\n",
    "                biases.append(b)        \n",
    "            return weights, biases\n",
    "            \n",
    "        def xavier_init(self, size):\n",
    "            in_dim = size[0]\n",
    "            out_dim = size[1]        \n",
    "            xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "            return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32, name = 'W')\n",
    "        \n",
    "        def neural_net(self, X, weights, biases):\n",
    "            num_layers = len(weights) + 1            \n",
    "            H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
    "            for l in range(0,num_layers-2):\n",
    "                W = weights[l]\n",
    "                b = biases[l]\n",
    "                H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "            W = weights[-1]\n",
    "            b = biases[-1]\n",
    "            Y = tf.add(tf.matmul(H, W), b)            \n",
    "            return Y\n",
    "                     \n",
    "        def net_uv(self, x, y, t):  \n",
    "            Y = self.neural_net(tf.concat([x,y,t],1), self.weights, self.biases)\n",
    "            u = Y[:, 0:1]\n",
    "            v = Y[:, 1:2]\n",
    "            return u,v\n",
    "        \n",
    "        def net_f(self, x, y, t, N_f):       \n",
    "            u, v = self.net_uv(x,y,t)\n",
    "            data = [u,v]            \n",
    "            \n",
    "            ## derivatives     \n",
    "            u_x = tf.gradients(u,x)[0]\n",
    "            u_xx = tf.gradients(u_x,x)[0]\n",
    "            u_y = tf.gradients(u,y)[0]\n",
    "            u_yy = tf.gradients(u_y,y)[0]\n",
    "            u_xy = tf.gradients(u_x,y)[0]\n",
    "            u_t = tf.gradients(u,t)[0]\n",
    "            \n",
    "            v_x = tf.gradients(v,x)[0]\n",
    "            v_xx = tf.gradients(v_x,x)[0]\n",
    "            v_y = tf.gradients(v,y)[0]\n",
    "            v_yy = tf.gradients(v_y,y)[0]\n",
    "            v_xy = tf.gradients(v_x,y)[0]\n",
    "            v_t = tf.gradients(v,t)[0]\n",
    "            \n",
    "            derivatives = [1]\n",
    "            derivatives.append(u_x)\n",
    "            derivatives.append(u_xx)\n",
    "            derivatives.append(u_y)\n",
    "            derivatives.append(u_yy)\n",
    "            derivatives.append(u_xy)\n",
    "            derivatives.append(v_x)\n",
    "            derivatives.append(v_xx)\n",
    "            derivatives.append(v_y)\n",
    "            derivatives.append(v_yy)\n",
    "            derivatives.append(v_xy)\n",
    "            \n",
    "            derivatives_description = ['', 'u_{x}', 'u_{xx}', 'u_{y}','u_{yy}','u_{xy}','v_{x}', 'v_{xx}', 'v_{y}','v_{yy}','v_{xy}']\n",
    "            \n",
    "            global lib_fun\n",
    "            global lib_descr\n",
    "            lib_fun, lib_descr = self.build_library(data, derivatives, derivatives_description, PolyOrder = 3, \n",
    "                                                    data_description = ['u','v'])      \n",
    "            f_u = u_t\n",
    "            f_v = v_t\n",
    "            Phi = tf.constant(1, shape=[N_f, 1], dtype=tf.float32)\n",
    "            for i in range(len(lib_fun)):\n",
    "                f_u = f_u - lib_fun[i]*self.lambda_u[i:i+1,0:1] # Note that the minus sign instead of the plus sign is uesd here.\n",
    "                f_v = f_v - lib_fun[i]*self.lambda_v[i:i+1,0:1] # Note that the minus sign instead of the plus sign is uesd here.\n",
    "                if i != 0:\n",
    "                    Phi = tf.concat([Phi, lib_fun[i]], 1)\n",
    "                                                \n",
    "            return f_u, f_v, Phi, u_t, v_t\n",
    "        \n",
    "        def build_library(self, data, derivatives, derivatives_description, PolyOrder, data_description = None):         \n",
    "            ## polynomial terms\n",
    "            P = PolyOrder\n",
    "            lib_poly = [1]\n",
    "            lib_poly_descr = [''] # it denotes '1'\n",
    "            for i in range(len(data)): # polynomial terms of univariable\n",
    "                for j in range(1, P+1):\n",
    "                    lib_poly.append(data[i]**j)\n",
    "                    lib_poly_descr.append(data_description[i]+\"**\"+str(j))\n",
    "                    \n",
    "            for i in range(1,P): # polynomial terms of bivariable. Assume we only have 2 variables.\n",
    "                for j in range(1,P-i+1):\n",
    "                    lib_poly.append(data[0]**i*data[1]**j)\n",
    "                    lib_poly_descr.append(data_description[0]+\"**\"+str(i)+data_description[1]+\"**\"+str(j))\n",
    "                    \n",
    "            ## derivative terms\n",
    "            lib_deri = derivatives\n",
    "            lib_deri_descr = derivatives_description\n",
    "            \n",
    "            ## Multiplication of derivatives and polynomials (including the multiplication with '1')\n",
    "            lib_poly_deri = []\n",
    "            lib_poly_deri_descr = []\n",
    "            for i in range(len(lib_poly)):\n",
    "                for j in range(len(lib_deri)):\n",
    "                    lib_poly_deri.append(lib_poly[i]*lib_deri[j])\n",
    "                    lib_poly_deri_descr.append(lib_poly_descr[i]+lib_deri_descr[j])\n",
    "                    \n",
    "            return lib_poly_deri,lib_poly_deri_descr\n",
    "                    \n",
    "        def callback(self, loss, lambda_u, lambda_v, loss_u, loss_v, loss_f_u, loss_f_v, loss_lambda_u, loss_lambda_v, loss_val):\n",
    "            global step\n",
    "            step = step+1\n",
    "            if step%10 == 0:\n",
    "                #### print out to files\n",
    "                print('It: %d, log Loss: %e, loss_u: %e, loss_v: %e, loss_f_u: %e, loss_f_v: %e, loss_lambda_u: %e, loss_lambda_v: %e, log loss_val: %e' % \n",
    "                      (step, loss, loss_u, loss_v, loss_f_u, loss_f_v, loss_lambda_u, loss_lambda_v, loss_val))\n",
    "                \n",
    "                global loss_history\n",
    "                global loss_val_history\n",
    "                global lambda_u_history\n",
    "                global lambda_v_history\n",
    "                global loss_u_history\n",
    "                global loss_v_history\n",
    "                global loss_f_u_history\n",
    "                global loss_f_v_history\n",
    "                global loss_lambda_u_history\n",
    "                global loss_lambda_v_history\n",
    "                \n",
    "                loss_history = np.append(loss_history, loss)\n",
    "                loss_val_history = np.append(loss_val_history, loss_val)\n",
    "                lambda_u_history = np.append(lambda_u_history, lambda_u, axis=1)\n",
    "                lambda_v_history = np.append(lambda_v_history, lambda_v, axis=1)\n",
    "                loss_u_history = np.append(loss_u_history, loss_u)\n",
    "                loss_v_history = np.append(loss_v_history, loss_v)\n",
    "                loss_f_u_history = np.append(loss_f_u_history, loss_f_u)\n",
    "                loss_f_v_history = np.append(loss_f_v_history, loss_f_v)\n",
    "                loss_lambda_u_history = np.append(loss_lambda_u_history, loss_lambda_u)\n",
    "                loss_lambda_v_history = np.append(loss_lambda_v_history, loss_lambda_v)\n",
    "                \n",
    "        def callback_Pretrain(self, loss, lambda_u, lambda_v, loss_u, loss_v, loss_f_u, loss_f_v, loss_lambda_u, loss_lambda_v,\n",
    "                              loss_val):\n",
    "            global step_Pretrain\n",
    "            step_Pretrain += 1\n",
    "            if step_Pretrain % 10 == 0:\n",
    "                #### print out to files\n",
    "                print('It: %d, log Loss: %e, loss_u: %e, loss_v: %e, loss_f_u: %e, loss_f_v: %e, loss_lambda_u: %e, loss_lambda_v: %e, log loss_val: %e' % \n",
    "                      (step_Pretrain, loss, loss_u, loss_v, loss_f_u, loss_f_v, loss_lambda_u, loss_lambda_v, loss_val))\n",
    "                \n",
    "                global loss_history_Pretrain\n",
    "                global loss_val_history_Pretrain\n",
    "                global lambda_u_history_Pretrain\n",
    "                global lambda_v_history_Pretrain\n",
    "                global loss_u_history_Pretrain\n",
    "                global loss_v_history_Pretrain\n",
    "                global loss_f_u_history_Pretrain\n",
    "                global loss_f_v_history_Pretrain\n",
    "                global loss_lambda_u_history_Pretrain\n",
    "                global loss_lambda_v_history_Pretrain\n",
    "                \n",
    "                loss_history_Pretrain = np.append(loss_history_Pretrain, loss)\n",
    "                loss_val_history_Pretrain = np.append(loss_val_history_Pretrain, loss_val)\n",
    "                lambda_u_history_Pretrain = np.append(lambda_u_history_Pretrain, lambda_u, axis=1)\n",
    "                lambda_v_history_Pretrain = np.append(lambda_v_history_Pretrain, lambda_v, axis=1)\n",
    "                loss_u_history_Pretrain = np.append(loss_u_history_Pretrain, loss_u)\n",
    "                loss_v_history_Pretrain = np.append(loss_v_history_Pretrain, loss_v)\n",
    "                loss_f_u_history_Pretrain = np.append(loss_f_u_history_Pretrain, loss_f_u)\n",
    "                loss_f_v_history_Pretrain = np.append(loss_f_v_history_Pretrain, loss_f_v)\n",
    "                loss_lambda_u_history_Pretrain = np.append(loss_lambda_u_history_Pretrain, loss_lambda_u)\n",
    "                loss_lambda_v_history_Pretrain = np.append(loss_lambda_v_history_Pretrain, loss_lambda_v)\n",
    "            \n",
    "        def Pretrain(self):       \n",
    "            # Dictionary        \n",
    "                        \n",
    "            # With batches\n",
    "            for i in range(self.BatchNo):                \n",
    "                x_batch = self.x[int(i*self.batchsize_train):int((i+1)*self.batchsize_train), :]\n",
    "                y_batch = self.y[int(i*self.batchsize_train):int((i+1)*self.batchsize_train), :]\n",
    "                t_batch = self.t[int(i*self.batchsize_train):int((i+1)*self.batchsize_train), :]\n",
    "                u_batch = self.u[int(i*self.batchsize_train):int((i+1)*self.batchsize_train), :]\n",
    "                v_batch = self.v[int(i*self.batchsize_train):int((i+1)*self.batchsize_train), :]\n",
    "                \n",
    "                x_f_batch = self.x_f[int(i*self.batchsize_f):int((i+1)*self.batchsize_f), :]\n",
    "                y_f_batch = self.y_f[int(i*self.batchsize_f):int((i+1)*self.batchsize_f), :]\n",
    "                t_f_batch = self.t_f[int(i*self.batchsize_f):int((i+1)*self.batchsize_f), :]\n",
    "                \n",
    "                x_val_batch = self.x_val[int(i*self.batchsize_val):int((i+1)*self.batchsize_val), :]\n",
    "                y_val_batch = self.y_val[int(i*self.batchsize_val):int((i+1)*self.batchsize_val), :]\n",
    "                t_val_batch = self.t_val[int(i*self.batchsize_val):int((i+1)*self.batchsize_val), :]\n",
    "                u_val_batch = self.u_val[int(i*self.batchsize_val):int((i+1)*self.batchsize_val), :]\n",
    "                v_val_batch = self.v_val[int(i*self.batchsize_val):int((i+1)*self.batchsize_val), :]\n",
    "                \n",
    "                self.tf_dict = {self.x_tf: x_batch, self.y_tf: y_batch, self.t_tf: t_batch, self.u_tf: u_batch, self.v_tf: v_batch,\n",
    "                                self.x_f_tf: x_f_batch, self.y_f_tf: y_f_batch, self.t_f_tf: t_f_batch, \n",
    "                                self.x_val_tf: x_val_batch, self.y_val_tf: y_val_batch, self.t_val_tf: t_val_batch,\n",
    "                                self.u_val_tf: u_val_batch, self.v_val_tf: v_val_batch,\n",
    "                                self.loss_f_coeff_tf:10}\n",
    "            \n",
    "                self.run_options = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "                # Adam optimizer(Pretraining)\n",
    "                print('Adam(Pretraining) starts')\n",
    "                start_time = time.time()\n",
    "                for it_Adam in range(Pre_Adam_iter): #10_000\n",
    "                    self.sess.run(self.train_op_Adam_Pretrain, self.tf_dict, options=self.run_options)\n",
    "                    \n",
    "                    # Print\n",
    "                    if it_Adam % 10 == 0:\n",
    "                        elapsed = time.time() - start_time\n",
    "                        loss, loss_u, loss_v, loss_f_u, loss_f_v, loss_lambda_u, loss_lambda_v, lambda_u, lambda_v, loss_val = \\\n",
    "                            self.sess.run([self.loss, self.loss_u, self.loss_v, self.loss_f_u, self.loss_f_v, self.loss_lambda_u, \n",
    "                                           self.loss_lambda_v, self.lambda_u, self.lambda_v, self.loss_val], self.tf_dict)\n",
    "                        print('It: %d, Log Loss: %e, loss_u: %e, loss_v: %e, loss_f_u: %e, loss_f_v: %e, loss_lambda_u: %e, loss_lambda_v: %e, loss_val: %e, Time: %.2f' \n",
    "                              % (it_Adam, loss, loss_u, loss_v, loss_f_u, loss_f_v, loss_lambda_u, \n",
    "                              loss_lambda_v, loss_val, elapsed))\n",
    "                        \n",
    "                        global loss_history_Adam_Pretrain\n",
    "                        global loss_val_history_Adam_Pretrain\n",
    "                        global lambda_u_history_Adam_Pretrain\n",
    "                        global lambda_v_history_Adam_Pretrain\n",
    "                        global loss_u_history_Adam_Pretrain\n",
    "                        global loss_v_history_Adam_Pretrain\n",
    "                        global loss_f_u_history_Adam_Pretrain\n",
    "                        global loss_f_v_history_Adam_Pretrain\n",
    "                        global loss_lambda_u_history_Adam_Pretrain\n",
    "                        global loss_lambda_v_history_Adam_Pretrain\n",
    "                        \n",
    "                        loss_history_Adam_Pretrain = np.append(loss_history_Adam_Pretrain, loss)\n",
    "                        loss_val_history_Adam_Pretrain = np.append(loss_val_history_Adam_Pretrain, loss_val)\n",
    "                        lambda_u_history_Adam_Pretrain = np.append(lambda_u_history_Adam_Pretrain, lambda_u, axis=1)\n",
    "                        lambda_v_history_Adam_Pretrain = np.append(lambda_v_history_Adam_Pretrain, lambda_v, axis=1)\n",
    "                        loss_u_history_Adam_Pretrain = np.append(loss_u_history_Adam_Pretrain, loss_u)\n",
    "                        loss_v_history_Adam_Pretrain = np.append(loss_v_history_Adam_Pretrain, loss_v)\n",
    "                        loss_f_u_history_Adam_Pretrain = np.append(loss_f_u_history_Adam_Pretrain, loss_f_u)\n",
    "                        loss_f_v_history_Adam_Pretrain = np.append(loss_f_v_history_Adam_Pretrain, loss_f_v)\n",
    "                        loss_lambda_u_history_Adam_Pretrain = np.append(loss_lambda_u_history_Adam_Pretrain, loss_lambda_u)\n",
    "                        loss_lambda_v_history_Adam_Pretrain = np.append(loss_lambda_v_history_Adam_Pretrain, loss_lambda_v)\n",
    "                \n",
    "                        start_time = time.time()\n",
    "                                                            \n",
    "                # L-BFGS-B optimizer(Pretraining)\n",
    "                print('L-BFGS-B(Pretraining) starts')\n",
    "                self.optimizer_Pretrain.minimize(self.sess,\n",
    "                                        feed_dict = self.tf_dict,\n",
    "                                        fetches = [self.loss, self.lambda_u, self.lambda_v, self.loss_u, self.loss_v, \n",
    "                                                   self.loss_f_u, self.loss_f_v, self.loss_lambda_u, self.loss_lambda_v,\n",
    "                                                   self.loss_val],\n",
    "                                        loss_callback = self.callback_Pretrain)\n",
    "                \n",
    "        def ADO(self, nIter):\n",
    "            # With batches\n",
    "            for i in range(self.BatchNo):                \n",
    "                x_batch = self.x[int(i*self.batchsize_train):int((i+1)*self.batchsize_train), :]\n",
    "                y_batch = self.y[int(i*self.batchsize_train):int((i+1)*self.batchsize_train), :]\n",
    "                t_batch = self.t[int(i*self.batchsize_train):int((i+1)*self.batchsize_train), :]\n",
    "                u_batch = self.u[int(i*self.batchsize_train):int((i+1)*self.batchsize_train), :]\n",
    "                v_batch = self.v[int(i*self.batchsize_train):int((i+1)*self.batchsize_train), :]\n",
    "                \n",
    "                x_f_batch = self.x_f[int(i*self.batchsize_f):int((i+1)*self.batchsize_f), :]\n",
    "                y_f_batch = self.y_f[int(i*self.batchsize_f):int((i+1)*self.batchsize_f), :]\n",
    "                t_f_batch = self.t_f[int(i*self.batchsize_f):int((i+1)*self.batchsize_f), :]\n",
    "                \n",
    "                x_val_batch = self.x_val[int(i*self.batchsize_val):int((i+1)*self.batchsize_val), :]\n",
    "                y_val_batch = self.y_val[int(i*self.batchsize_val):int((i+1)*self.batchsize_val), :]\n",
    "                t_val_batch = self.t_val[int(i*self.batchsize_val):int((i+1)*self.batchsize_val), :]\n",
    "                u_val_batch = self.u_val[int(i*self.batchsize_val):int((i+1)*self.batchsize_val), :]\n",
    "                v_val_batch = self.v_val[int(i*self.batchsize_val):int((i+1)*self.batchsize_val), :]\n",
    "                \n",
    "                self.tf_dict = {self.x_tf: x_batch, self.y_tf: y_batch, self.t_tf: t_batch, self.u_tf: u_batch, self.v_tf: v_batch,\n",
    "                                self.x_f_tf: x_f_batch, self.y_f_tf: y_f_batch, self.t_f_tf: t_f_batch, \n",
    "                                self.x_val_tf: x_val_batch, self.y_val_tf: y_val_batch, self.t_val_tf: t_val_batch,\n",
    "                                self.u_val_tf: u_val_batch, self.v_val_tf: v_val_batch,\n",
    "                                self.loss_f_coeff_tf: 10}\n",
    "                \n",
    "                self.run_options = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "\n",
    "                for self.it in range(nIter):    \n",
    "                    \n",
    "                    # STRidge optimizer\n",
    "                    print('STRidge starts')\n",
    "                    self.callTrainSTRidge()\n",
    "                    \n",
    "                    # Adam optimizer                \n",
    "                    print('Adam starts')\n",
    "                    start_time = time.time()\n",
    "                    for it_Adam in range(Adam_iter): #1_000\n",
    "                        self.sess.run(self.train_op_Adam, self.tf_dict, options=self.run_options)\n",
    "                        \n",
    "                        # Print\n",
    "                        if it_Adam % 10 == 0:\n",
    "                            elapsed = time.time() - start_time\n",
    "                            loss, loss_u, loss_v, loss_f_u, loss_f_v, loss_lambda_u, loss_lambda_v, lambda_u, lambda_v, loss_val = \\\n",
    "                                self.sess.run([self.loss, self.loss_u, self.loss_v, self.loss_f_u, self.loss_f_v, self.loss_lambda_u, \n",
    "                                               self.loss_lambda_v, self.lambda_u, self.lambda_v, self.loss_val], self.tf_dict)\n",
    "                            print('It: %d, Log Loss: %e, loss_u: %e, loss_v: %e, loss_f_u: %e, loss_f_v: %e, loss_lambda_u: %e, loss_lambda_v: %e, loss_val: %e, Time: %.2f' \n",
    "                                  % (it_Adam, loss, loss_u, loss_v, loss_f_u, loss_f_v, loss_lambda_u, \n",
    "                                  loss_lambda_v, loss_val, elapsed))\n",
    "                            \n",
    "                            global loss_history_Adam\n",
    "                            global loss_val_history_Adam\n",
    "                            global lambda_u_history_Adam\n",
    "                            global lambda_v_history_Adam\n",
    "                            global loss_u_history_Adam\n",
    "                            global loss_v_history_Adam\n",
    "                            global loss_f_u_history_Adam\n",
    "                            global loss_f_v_history_Adam\n",
    "                            global loss_lambda_u_history_Adam\n",
    "                            global loss_lambda_v_history_Adam\n",
    "                            \n",
    "                            loss_history_Adam = np.append(loss_history_Adam, loss)\n",
    "                            loss_val_history_Adam = np.append(loss_val_history_Adam, loss_val)\n",
    "                            lambda_u_history_Adam = np.append(lambda_u_history_Adam, lambda_u, axis=1)\n",
    "                            lambda_v_history_Adam = np.append(lambda_v_history_Adam, lambda_v, axis=1)\n",
    "                            loss_u_history_Adam = np.append(loss_u_history_Adam, loss_u)\n",
    "                            loss_v_history_Adam = np.append(loss_v_history_Adam, loss_v)\n",
    "                            loss_f_u_history_Adam = np.append(loss_f_u_history_Adam, loss_f_u)\n",
    "                            loss_f_v_history_Adam = np.append(loss_f_v_history_Adam, loss_f_v)\n",
    "                            loss_lambda_u_history_Adam = np.append(loss_lambda_u_history_Adam, loss_lambda_u)\n",
    "                            loss_lambda_v_history_Adam = np.append(loss_lambda_v_history_Adam, loss_lambda_v)\n",
    "                    \n",
    "                            start_time = time.time()\n",
    "                            \n",
    "\n",
    "                # STRidge optimizer\n",
    "                print('STRidge starts')\n",
    "                self.callTrainSTRidge()\n",
    "                \n",
    "        def predict(self, X_star):\n",
    "            \n",
    "            tf_dict = {self.x_tf: X_star[:,0:1], self.y_tf: X_star[:,1:2], self.t_tf: X_star[:,2:3]}\n",
    "            \n",
    "            u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "            v_star = self.sess.run(self.v_pred, tf_dict)\n",
    "            return u_star, v_star\n",
    "        \n",
    "        def callTrainSTRidge(self):\n",
    "            lam = 1e-5\n",
    "            d_tol = 1\n",
    "            maxit = 100\n",
    "            STR_iters = 10\n",
    "            \n",
    "            l0_penalty = None\n",
    "            \n",
    "            normalize = 2\n",
    "            split = 0.8\n",
    "            print_best_tol = False     \n",
    "            \n",
    "            # Process of lambda_u            \n",
    "            Phi, u_t_pred, v_t_pred = self.sess.run([self.Phi, self.u_t_pred, self.v_t_pred], self.tf_dict)        \n",
    "            lambda_u2, loss_u_history_STRidge2, loss_f_u_history_STRidge2, loss_lambda_u_history_STRidge2, tol_u_history_STRidge2, \\\n",
    "                optimaltol_u_history2 = self.TrainSTRidge(Phi, u_t_pred, lam, d_tol, maxit, STR_iters, l0_penalty, normalize,\n",
    "                                                          split, print_best_tol, uv_flag = True)\n",
    "            self.lambda_u = tf.assign(self.lambda_u, tf.convert_to_tensor(lambda_u2, dtype = tf.float32))\n",
    "                        \n",
    "            global loss_u_history_STRidge\n",
    "            global loss_f_u_history_STRidge\n",
    "            global loss_lambda_u_history_STRidge\n",
    "            global tol_u_history_STRidge\n",
    "            \n",
    "            loss_u_history_STRidge = np.append(loss_u_history_STRidge, loss_u_history_STRidge2)\n",
    "            loss_f_u_history_STRidge = np.append(loss_f_u_history_STRidge, loss_f_u_history_STRidge2)\n",
    "            loss_lambda_u_history_STRidge = np.append(loss_lambda_u_history_STRidge, loss_lambda_u_history_STRidge2)\n",
    "            tol_u_history_STRidge = np.append(tol_u_history_STRidge, tol_u_history_STRidge2)\n",
    "            \n",
    "            # Process of lambda_v    \n",
    "            lambda_v2, loss_v_history_STRidge2, loss_f_v_history_STRidge2, loss_lambda_v_history_STRidge2, tol_v_history_STRidge2, \\\n",
    "                optimaltol_v_history2 = self.TrainSTRidge(Phi, v_t_pred, lam, d_tol, maxit, STR_iters, l0_penalty, normalize,\n",
    "                                                          split, print_best_tol, uv_flag = False)\n",
    "            self.lambda_v = tf.assign(self.lambda_v, tf.convert_to_tensor(lambda_v2, dtype = tf.float32))\n",
    "                        \n",
    "            global loss_v_history_STRidge\n",
    "            global loss_f_v_history_STRidge\n",
    "            global loss_lambda_v_history_STRidge\n",
    "            global tol_v_history_STRidge\n",
    "            \n",
    "            loss_v_history_STRidge = np.append(loss_v_history_STRidge, loss_v_history_STRidge2)\n",
    "            loss_f_v_history_STRidge = np.append(loss_f_v_history_STRidge, loss_f_v_history_STRidge2)\n",
    "            loss_lambda_v_history_STRidge = np.append(loss_lambda_v_history_STRidge, loss_lambda_v_history_STRidge2)\n",
    "            tol_v_history_STRidge = np.append(tol_v_history_STRidge, tol_v_history_STRidge2)\n",
    "            \n",
    "                    \n",
    "        def TrainSTRidge(self, R, Ut, lam, d_tol, maxit, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, \n",
    "                         print_best_tol = False, uv_flag = True):            \n",
    "# =============================================================================\n",
    "#        Inspired by Rudy, Samuel H., et al. \"Data-driven discovery of partial differential equations.\"\n",
    "#        Science Advances 3.4 (2017): e1602614.\n",
    "# ============================================================================= \n",
    "        \n",
    "            # Split data into 80% training and 20% test, then search for the best tolderance.\n",
    "            np.random.seed(0) # for consistancy\n",
    "            n,_ = R.shape\n",
    "            train = np.random.choice(n, int(n*split), replace = False)\n",
    "            test = [i for i in np.arange(n) if i not in train]\n",
    "            TestR = R[test,:]\n",
    "            TestY = Ut[test,:]\n",
    "        \n",
    "            # Set up the initial tolerance and l0 penalty\n",
    "            d_tol = float(d_tol)\n",
    "            tol = d_tol\n",
    "        \n",
    "            # Get the standard least squares estimator            \n",
    "            if uv_flag:\n",
    "                w_best = self.sess.run(self.lambda_u)\n",
    "            else:\n",
    "                w_best = self.sess.run(self.lambda_v)\n",
    "                \n",
    "            # err_f = np.linalg.norm(TestY - TestR.dot(w_best), 2)\n",
    "            err_f = np.mean((TestY - TestR.dot(w_best))**2)\n",
    "            \n",
    "            if l0_penalty == None and self.it == 0: \n",
    "                # l0_penalty = 0.05*np.linalg.cond(R)\n",
    "                if uv_flag:\n",
    "                    self.l0_penalty_0_u = 10*err_f\n",
    "                    l0_penalty = self.l0_penalty_0_u\n",
    "                else:\n",
    "                    self.l0_penalty_0_v = 10*err_f\n",
    "                    l0_penalty = self.l0_penalty_0_v\n",
    "                    \n",
    "            elif l0_penalty == None:\n",
    "                if uv_flag:\n",
    "                    l0_penalty = self.l0_penalty_0_u\n",
    "                else:\n",
    "                    l0_penalty = self.l0_penalty_0_v\n",
    "            \n",
    "            err_lambda = l0_penalty*np.count_nonzero(w_best)\n",
    "            err_best = err_f + err_lambda\n",
    "            tol_best = 0\n",
    "                        \n",
    "            loss_history_STRidge = np.empty([0])\n",
    "            loss_f_history_STRidge = np.empty([0])\n",
    "            loss_lambda_history_STRidge = np.empty([0])\n",
    "            tol_history_STRidge = np.empty([0])\n",
    "            loss_history_STRidge = np.append(loss_history_STRidge, err_best)\n",
    "            loss_f_history_STRidge = np.append(loss_f_history_STRidge, err_f)\n",
    "            loss_lambda_history_STRidge = np.append(loss_lambda_history_STRidge, err_lambda)\n",
    "            tol_history_STRidge = np.append(tol_history_STRidge, tol_best)\n",
    "        \n",
    "            # Now increase tolerance until test performance decreases\n",
    "            for iter in range(maxit):\n",
    "        \n",
    "                # Get a set of coefficients and error\n",
    "                w = self.STRidge(R,Ut,lam,STR_iters,tol,normalize = normalize, uv_flag = uv_flag)\n",
    "                # err_f = np.linalg.norm(TestY - TestR.dot(w), 2)\n",
    "                err_f = np.mean((TestY - TestR.dot(w))**2)\n",
    "                \n",
    "                err_lambda = l0_penalty*np.count_nonzero(w)\n",
    "                err = err_f + err_lambda\n",
    "        \n",
    "                # Has the accuracy improved?\n",
    "                if err <= err_best:\n",
    "                    err_best = err\n",
    "                    w_best = w\n",
    "                    tol_best = tol\n",
    "                    tol = 1.2*tol\n",
    "                    \n",
    "                    loss_history_STRidge = np.append(loss_history_STRidge, err_best)\n",
    "                    loss_f_history_STRidge = np.append(loss_f_history_STRidge, err_f)\n",
    "                    loss_lambda_history_STRidge = np.append(loss_lambda_history_STRidge, err_lambda)\n",
    "                    tol_history_STRidge = np.append(tol_history_STRidge, tol)\n",
    "        \n",
    "                else:\n",
    "                    tol = 0.95*tol # 0.8*tol is the default\n",
    "        \n",
    "            if print_best_tol: print (\"Optimal tolerance:\", tol_best)\n",
    "            \n",
    "            optimaltol_history = np.empty([0])\n",
    "            optimaltol_history = np.append(optimaltol_history, tol_best)\n",
    "        \n",
    "            return np.real(w_best), loss_history_STRidge, loss_f_history_STRidge, loss_lambda_history_STRidge, tol_history_STRidge, optimaltol_history\n",
    "        \n",
    "        def STRidge(self, X0, y, lam, maxit, tol, normalize = 2, print_results = False, uv_flag = True):                 \n",
    "            n,d = X0.shape\n",
    "            X = np.zeros((n,d), dtype=np.complex64)\n",
    "            # First normalize data\n",
    "            if normalize != 0:\n",
    "                Mreg = np.zeros((d,1))\n",
    "                for i in range(0,d):\n",
    "                    Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize))\n",
    "                    X[:,i] = Mreg[i]*X0[:,i]\n",
    "            else: X = X0\n",
    "            \n",
    "            # Get the standard ridge esitmate            \n",
    "            # Inherit w from previous trainning\n",
    "            if uv_flag:\n",
    "                w = self.sess.run(self.lambda_u)/Mreg\n",
    "            else:\n",
    "                w = self.sess.run(self.lambda_v)/Mreg\n",
    "            \n",
    "            num_relevant = d\n",
    "            biginds = np.where( abs(w) > tol)[0]\n",
    "            \n",
    "            if uv_flag:\n",
    "                global ridge_u_append_counter_STRidge\n",
    "                ridge_u_append_counter = 0\n",
    "                \n",
    "                global lambda_u_history_STRidge\n",
    "                lambda_u_history_STRidge = np.append(lambda_u_history_STRidge, np.multiply(Mreg,w), axis = 1)\n",
    "                ridge_u_append_counter += 1\n",
    "            else:\n",
    "                global ridge_v_append_counter_STRidge\n",
    "                ridge_v_append_counter = 0\n",
    "                \n",
    "                global lambda_v_history_STRidge\n",
    "                lambda_v_history_STRidge = np.append(lambda_v_history_STRidge, np.multiply(Mreg,w), axis = 1)\n",
    "                ridge_v_append_counter += 1\n",
    "            \n",
    "            # Threshold and continue\n",
    "            for j in range(maxit):\n",
    "        \n",
    "                # Figure out which items to cut out\n",
    "                smallinds = np.where( abs(w) < tol)[0]\n",
    "                new_biginds = [i for i in range(d) if i not in smallinds]\n",
    "                    \n",
    "                # If nothing changes then stop\n",
    "                if num_relevant == len(new_biginds): break\n",
    "                else: num_relevant = len(new_biginds)\n",
    "                    \n",
    "                # Also make sure we didn't just lose all the coefficients\n",
    "                if len(new_biginds) == 0:\n",
    "                    if j == 0: \n",
    "                        if normalize != 0: \n",
    "                            w = np.multiply(Mreg,w)\n",
    "                            if uv_flag:\n",
    "                                lambda_u_history_STRidge = np.append(lambda_u_history_STRidge, w, axis = 1)\n",
    "                                ridge_u_append_counter += 1\n",
    "                                ridge_u_append_counter_STRidge = np.append(ridge_u_append_counter_STRidge, ridge_u_append_counter)\n",
    "                            else:\n",
    "                                lambda_v_history_STRidge = np.append(lambda_v_history_STRidge, w, axis = 1)\n",
    "                                ridge_v_append_counter += 1\n",
    "                                ridge_v_append_counter_STRidge = np.append(ridge_v_append_counter_STRidge, ridge_v_append_counter)\n",
    "                            return w\n",
    "                        else: \n",
    "                            if uv_flag:\n",
    "                                lambda_u_history_STRidge = np.append(lambda_u_history_STRidge, w, axis = 1)\n",
    "                                ridge_u_append_counter += 1\n",
    "                                ridge_u_append_counter_STRidge = np.append(ridge_u_append_counter_STRidge, ridge_u_append_counter)\n",
    "                            else:\n",
    "                                lambda_v_history_STRidge = np.append(lambda_v_history_STRidge, w, axis = 1)\n",
    "                                ridge_v_append_counter += 1\n",
    "                                ridge_v_append_counter_STRidge = np.append(ridge_v_append_counter_STRidge, ridge_v_append_counter)\n",
    "                            return w\n",
    "                    else: break\n",
    "                biginds = new_biginds\n",
    "                \n",
    "                # Otherwise get a new guess\n",
    "                w[smallinds] = 0\n",
    "                \n",
    "                if lam != 0: \n",
    "                    w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y))[0]\n",
    "                    if uv_flag:\n",
    "                        lambda_u_history_STRidge = np.append(lambda_u_history_STRidge, np.multiply(Mreg,w), axis = 1)\n",
    "                        ridge_u_append_counter += 1\n",
    "                    else:\n",
    "                        lambda_v_history_STRidge = np.append(lambda_v_history_STRidge, np.multiply(Mreg,w), axis = 1)\n",
    "                        ridge_v_append_counter += 1\n",
    "                else: \n",
    "                    w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n",
    "                    if uv_flag:\n",
    "                        lambda_u_history_STRidge = np.append(lambda_u_history_STRidge, np.multiply(Mreg,w), axis = 1)\n",
    "                        ridge_u_append_counter += 1\n",
    "                    else:\n",
    "                        lambda_v_history_STRidge = np.append(lambda_v_history_STRidge, np.multiply(Mreg,w), axis = 1)\n",
    "                        ridge_v_append_counter += 1\n",
    "    \n",
    "            # Now that we have the sparsity pattern, use standard least squares to get w\n",
    "            if biginds != []: \n",
    "                w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y))[0]\n",
    "            \n",
    "            if normalize != 0: \n",
    "                w = np.multiply(Mreg,w)\n",
    "                if uv_flag:\n",
    "                    lambda_u_history_STRidge = np.append(lambda_u_history_STRidge, w, axis = 1)\n",
    "                    ridge_u_append_counter += 1\n",
    "                    ridge_u_append_counter_STRidge = np.append(ridge_u_append_counter_STRidge, ridge_u_append_counter)\n",
    "                else:\n",
    "                    lambda_v_history_STRidge = np.append(lambda_v_history_STRidge, w, axis = 1)\n",
    "                    ridge_v_append_counter += 1\n",
    "                    ridge_v_append_counter_STRidge = np.append(ridge_v_append_counter_STRidge, ridge_v_append_counter)\n",
    "                return w\n",
    "            else:\n",
    "                if uv_flag:\n",
    "                    lambda_u_history_STRidge = np.append(lambda_u_history_STRidge, w, axis = 1)\n",
    "                    ridge_u_append_counter += 1\n",
    "                    ridge_u_append_counter_STRidge = np.append(ridge_u_append_counter_STRidge, ridge_u_append_counter)\n",
    "                else:\n",
    "                    lambda_v_history_STRidge = np.append(lambda_v_history_STRidge, w, axis = 1)\n",
    "                    ridge_v_append_counter += 1\n",
    "                    ridge_v_append_counter_STRidge = np.append(ridge_v_append_counter_STRidge, ridge_v_append_counter)\n",
    "                return w\n",
    "    def preprocess_test(filename):\n",
    "        data = np.load(filename)\n",
    "\n",
    "        t = data[:, 2].flatten()[:,None]/24\n",
    "        x = data[:, 0].flatten()[:,None]\n",
    "        y = data[:, 1].flatten()[:,None]\n",
    "        u = data[:, 3].flatten()[:,None]\n",
    "        v = data[:, 4].flatten()[:,None]     \n",
    "\n",
    "\n",
    "        X_star = np.hstack((x, y, t))\n",
    "        u_star = u\n",
    "        v_star = v\n",
    "\n",
    "        # Doman bounds\n",
    "        lb = X_star.min(0)\n",
    "        ub = X_star.max(0)    \n",
    "\n",
    "\n",
    "        # Select some points as training/val data\n",
    "        N_uv_s = max(X_star.shape[0], 5000*15)  #2500*15 #the number 15 here is a reference to the number of time steps in the original code. 12_000_000 is roughly the max number of points to avoid OOM error\n",
    "        print(f\"N_uv_s: {N_uv_s}\")\n",
    "        idx = np.random.choice(X_star.shape[0], N_uv_s, replace=False)\n",
    "\n",
    "        X_star_meas = X_star[idx, :]\n",
    "        u_star_meas = u_star[idx, :]\n",
    "        v_star_meas = v_star[idx, :]\n",
    "\n",
    "\n",
    "        # Training measurements, which are randomly sampled spatio-temporally\n",
    "        Split_TrainVal = 0.8\n",
    "        N_u_train = int(X_star_meas.shape[0]*Split_TrainVal)\n",
    "        idx_train = np.random.choice(X_star_meas.shape[0], N_u_train, replace=False)\n",
    "\n",
    "\n",
    "        X_star_train = X_star_meas[idx_train,:]\n",
    "        u_star_train = u_star_meas[idx_train,:]\n",
    "        v_star_train = v_star_meas[idx_train,:]\n",
    "\n",
    "        # Validation Measurements, which are the rest of measurements\n",
    "        idx_val = np.setdiff1d(np.arange(X_star_meas.shape[0]), idx_train, assume_unique=True)\n",
    "\n",
    "        X_star_val = X_star_meas[idx_val,:]\n",
    "        u_star_val = u_star_meas[idx_val,:]\n",
    "        v_star_val = v_star_meas[idx_val,:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Collocation points\n",
    "        N_f = 80000\n",
    "        X_f = lb + (ub-lb)*lhs(3, N_f)\n",
    "\n",
    "\n",
    "        BatchNo = 1\n",
    "\n",
    "        X_star_train = X_star_train.astype(np.float32)\n",
    "        u_star_train = u_star_train.astype(np.float32)\n",
    "        v_star_train = v_star_train.astype(np.float32)\n",
    "        X_f = X_f.astype(np.float32)\n",
    "        X_star_val = X_star_val.astype(np.float32)\n",
    "        u_star_val = u_star_val.astype(np.float32)\n",
    "        v_star_val = v_star_val.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "        return X_star, X_star_train, u_star_train, v_star_train, X_star_val, u_star_val, v_star_val, X_f, lb, ub\n",
    "        \n",
    "    if __name__ == \"__main__\": \n",
    "         \n",
    "        start_time = time.time()\n",
    "        \n",
    "        layers = [3] + 8*[60] + [2]\n",
    "\n",
    "        \n",
    "        X_star, X_star_train, u_star_train, v_star_train, X_star_val, u_star_val, v_star_val, X_f, lb, ub = preprocess_test(data_file)\n",
    "        BatchNo = 1 \n",
    "# =============================================================================\n",
    "#         train model\n",
    "# =============================================================================\n",
    "        model = PhysicsInformedNN(X_star_train, u_star_train, v_star_train, X_f, X_star_val, u_star_val, v_star_val, layers, lb, ub, BatchNo)\n",
    "        model.Pretrain()\n",
    "        \n",
    "        # Checkpoint model        \n",
    "        saved_path = model.saver.save(model.sess, data_out_folder + '/saved_variable')\n",
    "        # print('model saved in {}'.format(saved_path))                \n",
    "        \n",
    "        ## Restore pretrained model\n",
    "#        # in case we need to create graph \n",
    "        # model = PhysicsInformedNN(X_star_train, u_star_train, v_star_train, X_f, X_star_val, u_star_val, v_star_val, layers, lb, ub, BatchNo)\n",
    "#        # restore the saved vairable\n",
    "        # model.saver.restore(model.sess, data_out_folder + '/saved_variable')\n",
    "        \n",
    "        model.ADO(ADO_CYCLES)\n",
    "        \n",
    "        saved_path = model.saver.save(model.sess, data_out_folder + '/saved_variable_ADO')\n",
    "        # model.saver.restore(model.sess, data_out_folder + '/saved_variable_ADO')\n",
    "# =============================================================================\n",
    "#         check if training efforts are sufficient\n",
    "# =============================================================================\n",
    "        elapsed = time.time() - start_time   \n",
    "        f = open(data_out_folder + \"/stdout.txt\", \"w\")                         \n",
    "        f.write('Training time: %.4f \\n' % (elapsed))\n",
    "        \n",
    "        u_train_pred, v_train_pred = model.predict(X_star_train)\n",
    "        error_u_train = np.linalg.norm(u_star_train-u_train_pred,2)/np.linalg.norm(u_star_train,2)        \n",
    "        error_v_train = np.linalg.norm(v_star_train-v_train_pred,2)/np.linalg.norm(v_star_train,2)\n",
    "        f.write('Training Error u: %e \\n' % (error_u_train))    \n",
    "        f.write('Training Error v: %e \\n' % (error_v_train))   \n",
    "        \n",
    "        u_val_pred, v_val_pred = model.predict(X_star_val)\n",
    "        error_u_val = np.linalg.norm(u_star_val-u_val_pred,2)/np.linalg.norm(u_star_val,2)        \n",
    "        error_v_val = np.linalg.norm(v_star_val-v_val_pred,2)/np.linalg.norm(v_star_val,2)\n",
    "        f.write('Val Error u: %e \\n' % (error_u_val))    \n",
    "        f.write('Val Error v: %e \\n' % (error_v_val))   \n",
    "        \n",
    "        f.close()\n",
    "        ######################## Plots for Adam(Pretraining) #################\n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_history_Adam_Pretrain)\n",
    "        plt.plot(loss_val_history_Adam_Pretrain)\n",
    "        plt.legend(('train loss', 'val loss'))\n",
    "        plt.xlabel('10x')\n",
    "        plt.title('log loss history of Adam(Pretraining)')\n",
    "        plt.savefig(images_out_folder + '/1.png')\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_u_history_Adam_Pretrain)\n",
    "        plt.plot(loss_v_history_Adam_Pretrain)\n",
    "        plt.yscale('log')       \n",
    "        plt.xlabel('10x')\n",
    "        plt.legend(('loss_u', 'loss_v'))\n",
    "        plt.title('loss_u and loss_v history of Adam(Pretraining)')  \n",
    "        plt.savefig(images_out_folder + '/2.png')\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_f_u_history_Adam_Pretrain)\n",
    "        plt.plot(loss_f_v_history_Adam_Pretrain)\n",
    "        plt.yscale('log')       \n",
    "        plt.xlabel('10x')\n",
    "        plt.legend(('loss_f_u', 'loss_f_v'))\n",
    "        plt.title('loss_f_u and loss_f_v history of Adam(Pretraining)')  \n",
    "        plt.savefig(images_out_folder + '/3.png')\n",
    "                \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_lambda_u_history_Adam_Pretrain)\n",
    "        plt.plot(loss_lambda_v_history_Adam_Pretrain)\n",
    "        plt.yscale('log')       \n",
    "        plt.xlabel('10x')\n",
    "        plt.legend(('loss_lambda_u', 'loss_lambda_v'))\n",
    "        plt.title('loss_lambda_u and loss_lambda_v history of Adam(Pretraining)')  \n",
    "        plt.savefig(images_out_folder + '/4.png')\n",
    "        \n",
    "        ######################## Plots for L-BFGS-B(Pretraining) #################\n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_history_Pretrain)\n",
    "        plt.plot(loss_val_history_Pretrain)\n",
    "        plt.legend(('train loss', 'val loss'))\n",
    "        plt.xlabel('10x')\n",
    "        plt.title('log loss history of BFGS(Pretrain)')  \n",
    "        plt.savefig(images_out_folder + '/5.png')\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_u_history_Pretrain)\n",
    "        plt.plot(loss_v_history_Pretrain)\n",
    "        plt.yscale('log')       \n",
    "        plt.xlabel('10x')\n",
    "        plt.legend(('loss_u', 'loss_v'))\n",
    "        plt.title('loss_u and loss_v history of BFGS(Pretrain)')  \n",
    "        plt.savefig(images_out_folder + '/6.png')\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_f_u_history_Pretrain)\n",
    "        plt.plot(loss_f_v_history_Pretrain)\n",
    "        plt.yscale('log')       \n",
    "        plt.xlabel('10x')\n",
    "        plt.legend(('loss_f_u', 'loss_f_v'))\n",
    "        plt.title('loss_f_u and loss_f_v history of BFGS(Pretrain)')     \n",
    "        plt.savefig(images_out_folder + '/7.png')\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_lambda_u_history_Pretrain)\n",
    "        plt.plot(loss_lambda_v_history_Pretrain)\n",
    "        plt.yscale('log')       \n",
    "        plt.xlabel('10x')\n",
    "        plt.legend(('loss_lambda_u', 'loss_lambda_v'))\n",
    "        plt.title('loss_lambda_u and loss_lambda_v history of BFGS(Pretrain)') \n",
    "        plt.savefig(images_out_folder + '/8.png')\n",
    "    \n",
    "        ######################## Plots for Adam #################\n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_history_Adam)\n",
    "        plt.plot(loss_val_history_Adam)\n",
    "        plt.legend(('train loss', 'val loss'))\n",
    "        plt.xlabel('10x')\n",
    "        plt.title('log loss history of Adam')\n",
    "        plt.savefig(images_out_folder + '/9.png')\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_u_history_Adam)\n",
    "        plt.plot(loss_v_history_Adam)\n",
    "        plt.yscale('log')       \n",
    "        plt.xlabel('10x')\n",
    "        plt.legend(('loss_u', 'loss_v'))\n",
    "        plt.title('loss_u and loss_v history of Adam')  \n",
    "        plt.savefig(images_out_folder + '/10.png')\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_f_u_history_Adam)\n",
    "        plt.plot(loss_f_v_history_Adam)\n",
    "        plt.yscale('log')       \n",
    "        plt.xlabel('10x')\n",
    "        plt.legend(('loss_f_u', 'loss_f_v'))\n",
    "        plt.title('loss_f_u and loss_f_v history of Adam') \n",
    "        plt.savefig(images_out_folder + '/11.png')\n",
    "                \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_lambda_u_history_Adam)\n",
    "        plt.plot(loss_lambda_v_history_Adam)\n",
    "        plt.yscale('log')       \n",
    "        plt.xlabel('10x')\n",
    "        plt.legend(('loss_lambda_u', 'loss_lambda_v'))\n",
    "        plt.title('loss_lambda_u and loss_lambda_v history of Adam')  \n",
    "        plt.savefig(images_out_folder + '/12.png')\n",
    "                \n",
    "        ######################## Plots for BFGS #################\n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_history)\n",
    "        plt.plot(loss_val_history)\n",
    "        plt.legend(('train loss', 'val loss'))\n",
    "        plt.xlabel('10x')\n",
    "        plt.title('log loss history of BFGS')  \n",
    "        plt.savefig(images_out_folder + '/13.png')\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_u_history)\n",
    "        plt.plot(loss_v_history)\n",
    "        plt.yscale('log')       \n",
    "        plt.xlabel('10x')\n",
    "        plt.legend(('loss_u', 'loss_v'))\n",
    "        plt.title('loss_u and loss_v history of BFGS')  \n",
    "        plt.savefig(images_out_folder + '/14.png')\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_f_u_history)\n",
    "        plt.plot(loss_f_v_history)\n",
    "        plt.yscale('log')       \n",
    "        plt.xlabel('10x')\n",
    "        plt.legend(('loss_f_u', 'loss_f_v'))\n",
    "        plt.title('loss_f_u and loss_f_v history of BFGS')     \n",
    "        plt.savefig(images_out_folder + '/15.png')\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_lambda_u_history)\n",
    "        plt.plot(loss_lambda_v_history)\n",
    "        plt.yscale('log')       \n",
    "        plt.xlabel('10x')\n",
    "        plt.legend(('loss_lambda_u', 'loss_lambda_v'))\n",
    "        plt.title('loss_lambda_u and loss_lambda_v history of BFGS')  \n",
    "        plt.savefig(images_out_folder + '/16.png')\n",
    "        \n",
    "        ######################## Plots for STRidge #################        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_u_history_STRidge)\n",
    "        plt.plot(loss_v_history_STRidge)\n",
    "        plt.yscale('log')       \n",
    "        plt.legend(('u', 'v'))\n",
    "        plt.title('History of STRidge(loss_f+loss_lambda )')  \n",
    "        plt.savefig(images_out_folder + '/17.png')\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_f_u_history_STRidge)\n",
    "        plt.plot(loss_f_v_history_STRidge)\n",
    "        plt.yscale('log')       \n",
    "        plt.legend(('loss_f_u', 'loss_f_v'))\n",
    "        plt.title('loss_f_u and loss_f_v history of STRidge') \n",
    "        plt.savefig(images_out_folder + '/18.png')\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_lambda_u_history_STRidge)\n",
    "        plt.plot(loss_lambda_v_history_STRidge)\n",
    "        plt.yscale('log')      \n",
    "        plt.legend(('loss_lambda_u', 'loss_lambda_v'))\n",
    "        plt.title('loss_lambda_u and loss_lambda_v history of STRidge')  \n",
    "        plt.savefig(images_out_folder + '/19.png')\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(tol_u_history_STRidge)\n",
    "        plt.plot(tol_v_history_STRidge)\n",
    "        plt.legend(('tol_u', 'tol_v'))\n",
    "        plt.title('tol_u and tol_v history of STRidge')  \n",
    "        plt.savefig(images_out_folder + '/20.png')\n",
    "        \n",
    "        ######################## Plots for Alter #################\n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_history_Alter)\n",
    "        plt.plot(loss_val_history_Alter)\n",
    "        plt.legend(('train loss', 'val loss'))\n",
    "        plt.xlabel('10x')\n",
    "        plt.title('log loss history of Alter')\n",
    "        plt.savefig(images_out_folder + '/21.png')\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_u_history_Alter)\n",
    "        plt.plot(loss_v_history_Alter)\n",
    "        plt.yscale('log')       \n",
    "        plt.xlabel('10x')\n",
    "        plt.legend(('loss_u', 'loss_v'))\n",
    "        plt.title('loss_u and loss_v history of Alter')  \n",
    "        plt.savefig(images_out_folder + '/22.png')\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_f_u_history_Alter)\n",
    "        plt.plot(loss_f_v_history_Alter)\n",
    "        plt.yscale('log')       \n",
    "        plt.xlabel('10x')\n",
    "        plt.legend(('loss_f_u', 'loss_f_v'))\n",
    "        plt.title('loss_f_u and loss_f_v history of Alter') \n",
    "        plt.savefig(images_out_folder + '/23.png')\n",
    "                \n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_lambda_u_history_Alter)\n",
    "        plt.plot(loss_lambda_v_history_Alter)\n",
    "        plt.yscale('log')       \n",
    "        plt.xlabel('10x')\n",
    "        plt.legend(('loss_lambda_u', 'loss_lambda_v'))\n",
    "        plt.title('loss_lambda_u and loss_lambda_v history of Alter')  \n",
    "        plt.savefig(images_out_folder + '/24.png')\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129c8af7-824d-40fa-88d5-cc9e9a20fdfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c2a903-b1af-4ea5-821e-f53e746ef129",
   "metadata": {
    "tags": []
   },
   "source": [
    "## u and v measurements for train_val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd9926-82e9-4557-90f9-3521762926b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_animation(x, y, ground_truth, prediction, n_frames, file_name, lb=-3, ub=3, suptitle=\"free_run Daily dataset\", title1=\"U-HYCOM\", title2=\"U-DNN+TL\"):\n",
    "    \n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "\n",
    "    image_c_1 = ax1.pcolormesh(x[:,:,0], y[:,:,0], ground_truth[:,:,0], cmap='jet', vmin=lb, vmax=ub)\n",
    "\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.5)\n",
    "    fig.colorbar(image_c_1, cax=cax, orientation='vertical')\n",
    "    \n",
    "    image_c_2 = ax2.pcolormesh(x[:,:,0], y[:,:,0], prediction[:,:,0], cmap='jet', vmin=lb, vmax=ub)\n",
    "\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.5)\n",
    "    fig.colorbar(image_c_2, cax=cax, orientation='vertical');\n",
    "    \n",
    "    def animate(i):\n",
    "        image_c_1.set_array(ground_truth[:,:,i].flatten())\n",
    "        ax1.set_title(f\"{title1}, frame={i+1}\", fontsize=10)\n",
    "        image_c_2.set_array(prediction[:,:,i].flatten())\n",
    "        ax2.set_title(f\"{title2}, frame={i+1}\", fontsize=10)\n",
    "        return (image_c_1,image_c_2)\n",
    "    \n",
    "    ani = animation.FuncAnimation(fig, animate, frames=n_frames, interval = 50, blit = False)\n",
    "    fig.suptitle(suptitle)\n",
    "    ani.save(images_out_folder+\"/\"+file_name+\".gif\", fps = 1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa306aee-9d58-41b4-bcd7-fb6288126742",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Plotting Assimilated data (Ground truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ceda98-7f3d-4a2b-8748-625b1bc1ff86",
   "metadata": {
    "tags": []
   },
   "source": [
    "## u and v velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74017e8-c1e6-490f-ba82-502dfa8f4196",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lat = np.squeeze(scipy.io.loadmat(\"data/model_lat.mat\")['model_lat'])\n",
    "model_lon = np.squeeze(scipy.io.loadmat(\"data/model_lon.mat\")['model_lon'])\n",
    "grid_velocity = scipy.io.loadmat(\"data/grid_velocity_truth.mat\")['grid_velocity']\n",
    "\n",
    "\n",
    "# GoM Region\n",
    "Longitude_min = -98\n",
    "Longitude_max = -76.4\n",
    "Latitude_min = 18.09\n",
    "Latitude_max = 31.96\n",
    "\n",
    "\n",
    "\n",
    "Latitude_min_coor = np.squeeze(np.argwhere(model_lat>=Latitude_min))\n",
    "Latitude_max_coor = np.squeeze(np.argwhere(model_lat<=Latitude_max))\n",
    "Longitude_min_coor = np.squeeze(np.argwhere(model_lon>=Longitude_min))\n",
    "Longitude_max_coor = np.squeeze(np.argwhere(model_lon<=Longitude_max))\n",
    "\n",
    "u_grided = np.squeeze(grid_velocity[0, :, Longitude_min_coor[0]:Longitude_max_coor[-1],Latitude_min_coor[0]:Latitude_max_coor[-1]])\n",
    "v_grided = np.squeeze(grid_velocity[1, :, Longitude_min_coor[0]:Longitude_max_coor[-1],Latitude_min_coor[0]:Latitude_max_coor[-1]])\n",
    "\n",
    "u_grided = u_grided.transpose(1, 2, 0)\n",
    "v_grided = v_grided.transpose(1, 2, 0)\n",
    "\n",
    "model_lat=model_lat[Latitude_min_coor[0]:Latitude_max_coor[-1]];\n",
    "model_lon=model_lon[Longitude_min_coor[0]:Longitude_max_coor[-1]];\n",
    "\n",
    "\n",
    "t = np.real(np.arange(1, 30+1).flatten()[:,None])[:N_FRAMES, :]  # 30 represents a month worth of data given the daily sampling rate\n",
    "x = np.real(model_lon.flatten()[:,None])\n",
    "y = np.real(model_lat.flatten()[:,None])\n",
    "Exact_u = u_grided[..., :N_FRAMES]\n",
    "Exact_v = v_grided[..., :N_FRAMES]\n",
    "\n",
    "\n",
    "X, Y, T = np.meshgrid(x, y, t, indexing='ij')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b901df-90ea-4bae-b0a5-3e994b67a84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_b = -2 # min(np.nanmin(Exact_u), np.nanmin(Exact_v))\n",
    "u_b = 2 # max(np.nanmax(Exact_u), np.nanmax(Exact_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6079f101-9a9e-44a3-90c9-819bce854744",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animation(X, Y, Exact_u, Exact_v, n_frames=N_FRAMES, file_name=\"U_V_Assimilated_data_set\", lb=l_b, ub=u_b, title1=\"U-Assimilated_data_set\", title2=\"V-Assimilated_data_set\", suptitle=\"Assimilated data daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378c31b9-b2c3-4498-b0b4-9149d6660c1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## u_true and u_pred animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626ae17-aef6-4b3d-9c06-da8d18c09cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_star = np.hstack((X.flatten()[:,None], Y.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact_u.flatten()[:,None] \n",
    "v_star = Exact_v.flatten()[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc27c9d8-7290-42df-9808-f0640a4ad309",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred, v_pred = model.predict(X_star) \n",
    "\n",
    "u_pred_reshaped = u_pred.reshape(X.shape)\n",
    "v_pred_reshaped = v_pred.reshape(X.shape)\n",
    "\n",
    "# The following two lines help to add NaN values to the prediction for the case when we use the entire GOM region which also involves ground, i.e., non ocean areas\n",
    "u_pred_reshaped = ((u_pred_reshaped - Exact_u) + Exact_u)\n",
    "v_pred_reshaped = ((v_pred_reshaped - Exact_v) + Exact_v)\n",
    "\n",
    "scipy.io.savemat(data_out_folder+'/DNN_output.mat', {'u_pred':u_pred_reshaped, 'v_pred':v_pred_reshaped})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e803b1-eb4f-40e5-b039-ca206caf2a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_b = -2 # min(Exact_u.min(), u_pred.min())\n",
    "u_b = 2 #max(Exact_u.max(), u_pred.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c0a07-3edf-4bf1-9401-4ab9c7e6fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animation(X, Y, Exact_u, u_pred_reshaped, n_frames=N_FRAMES, file_name=\"U_true_pred_comparison\", \n",
    "               lb=l_b, ub=u_b, title1=\"U-Assimilated\", title2=\"U-DNN-output\", suptitle=\"Assimilated data daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc1bd2-efff-4a74-8fd2-17bd4a058016",
   "metadata": {
    "tags": []
   },
   "source": [
    "## v_true and v_pred animation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f26cb-483d-4cd4-ae6e-5819c815d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_b = -2 # min(np.nanmin(Exact_v), np.nanmin(v_pred))\n",
    "u_b = 2 # max(np.nanmax(Exact_v), np.nanmax(v_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a9cbd8-3270-4d4b-b023-2bb2116a18e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animation(X, Y, Exact_v, v_pred_reshaped, n_frames=N_FRAMES, file_name=\"V_true_pred_comparison\", lb=l_b, ub=u_b, title1=\"V_True\",\n",
    "               title2=\"V_Pred\", suptitle=\"Assimilated data - PINN output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d051a58-ff1c-43cb-8356-a43f4dcb3128",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Point to point difference between true and PINN output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a0409-3dc2-4be4-ae08-3c4a5cb55f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_u = Exact_u - u_pred_reshaped\n",
    "diff_v = Exact_v - v_pred_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a3b19-e41d-4ec4-8d8d-dc1140e81cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_b = -2 # min(np.nanmin(Exact_u), np.nanmin(u_pred))\n",
    "u_b = 2 # max(np.nanmax(Exact_u), np.nanmax(u_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baebf10a-d7c6-4973-b726-6c14f69d9a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animation(X, Y, diff_u, diff_v, n_frames=N_FRAMES, file_name=\"UV_true_pred_diff\", lb=l_b, ub=u_b, title1=\"Difference_U_true_U-PINN\", title2=\"Difference_V_true_V-PINN\", suptitle=\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27857f85-3ef6-4d59-ae79-15132879960a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
